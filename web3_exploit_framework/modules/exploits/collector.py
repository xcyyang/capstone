from collections import defaultdict
import json
import os
from pprint import pprint
import yaml
import csv

root_path = os.getcwd()
poc_path = os.path.join("web3_exploit_framework", "modules","exploits")
cfg_name = "config.yml"
csv_path = os.path.join("web3_exploit_framework", "modules","exploits")
csv_name = "pocs.csv"

header = ["name", "date", "desc", "proj", "chain", "vuln", "exp", "root_cause"]

# read single project's config file to retrieve the basic information
def read_single_config(proj_path):
    filename = os.path.join(proj_path, cfg_name)
    data = []
    with open(filename, "r") as stream:
        try:
            content = yaml.safe_load(stream)
            _info = content["info"]
            _tags = _info["tags"]
            data = [ 
                _info["description"],           # desc
                ','.join(_tags["project"]) if _tags["project"] else None,     # proj
                ','.join(_tags["chains"]) if _tags["chains"] else None,      # chain
                ','.join(_tags["vulnerabilities"]) if _tags["vulnerabilities"] else None, # vuln
                ','.join(_tags["exploits"]) if _tags["exploits"] else None,     # exp
                _info["root_cause"]
            ]    
        except yaml.YAMLError as exc:
            print(exc)
        except Exception as exc:
            print(proj_path, 'error')
            print(exc)
    return data

# get single project's name, date and basic information
def handle_single_exploit(proj_path):
    base_name = os.path.basename(proj_path)
    res = base_name.rsplit("_", 1) # [_name, _date]
    _data = read_single_config(proj_path)
    res.extend(_data)
    return res

# batch handle all of exploits
def handle_exploits():
    full_path = os.path.join(root_path, poc_path)
    file_list = os.listdir(full_path)

    csv_data = []
    for _path in file_list:
        cur_path = os.path.join(full_path, _path)
        if not os.path.isdir(cur_path): continue 
        _data = handle_single_exploit(cur_path)
        csv_data.append(_data)
    
    return csv_data

# write all projects' info to a csv file
def write_csv(data):
    full_path = os.path.join(root_path, csv_path, csv_name)
    with open(full_path, 'w', encoding='UTF8', newline='') as f:
        writer = csv.writer(f, delimiter='\t')
        writer.writerow(header)
        writer.writerows(data)

# append certain data to the csv file
def append_csv(data):
    full_path = os.path.join(root_path, csv_path, csv_name)
    with open(full_path, 'w', encoding='UTF8', newline='') as f:
        writer = csv.writer(f, delimiter='\t')
        writer.writerow(data)

# generate the PoCs csv file
def generate_csv():
    data = handle_exploits()
    write_csv(data)

# update the PoCs csv file
def update_csv():
    pass

def get_statistics():
    key = ["desc", "proj", "chain", "vuln", "exp", "root_cause"]

    full_path = os.path.join(root_path, poc_path)
    file_list = os.listdir(full_path)

    stat_dict = {
        "chain": defaultdict(int),
        "proj": defaultdict(int),
        "vuln": defaultdict(int),
        "exp": defaultdict(int),
        "root_cause": defaultdict(int)
    }
    for _path in file_list:
        cur_path = os.path.join(full_path, _path)
        if not os.path.isdir(cur_path): continue 
        _, _, _, _proj, _chain, _vuln, _exp, _root_cause = handle_single_exploit(cur_path)
        if _proj:
            for item in _proj.split(','):
                stat_dict["proj"][item] += 1
        if _chain:
            for item in _chain.split(','):
                stat_dict["chain"][item] += 1
        if _vuln:
            for item in _vuln.split(','):
                stat_dict["vuln"][item] += 1
        if _exp:
            for item in _exp.split(','):
                stat_dict["exp"][item] += 1
        stat_dict["root_cause"][_root_cause] += 1 
    
    with open(os.path.join(full_path, "stat.json"), 'w') as f:
        line = json.dumps(stat_dict)
        f.write(line)

if __name__ == "__main__":
    generate_csv()
    get_statistics()